\section{Introduction}\label{introduction}
Access and retrieval mechanisms for archives and museums typically rely on a rich description of the collection. 
Most cultural heritage institutions employ professional experts to describe their collections by manually compiling metadata for each item. 
The subject matter of collection items can be very diverse, think for example of historic figures, animals, plants and buildings. Additionaly, these aspects often carry a symbolic meaning.
To adequately describe items in large and diverse collections, the knowledge of experts from other domains is indispensable.
Cultural heritage institutions therefore seek to understand whether and how they can make use of external users to produce these annotations.

The work we present aims at understanding which strategies and techniques lead to high-quality annotations by (crowds of) users that are external to the museum. 
The first challenge in the project is to identify the niche of relevant experts and to motivate them to contribute to the annotation of collection items. 
Next, personalization mechanisms must make sure that the experts are shown items that correspond to their expertise. 
The quality of the annotations and annotators is to be evaluated using algorithms considering trust. As a final challenge, all these aspects must be presented in an appropriate interface.

To evaluate our hypotheses, we develop a framework to support crowd annotation processes, called Accurator. 