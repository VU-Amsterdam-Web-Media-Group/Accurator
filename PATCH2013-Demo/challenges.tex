\section{Research Challenges}
\label{use_case}

One of the four main challenges of nichesourcing is finding candidate annotators that are able to produce high quality annotations for collection items. 
%Besides topical knowledge, properties like availability, willingness to help and being able to share or transfer knowledge are also important. 
We believe that people participating in a topical community have an active interest in that topic and might be willing to help and share knowledge related it. We refer to these topical communities as \textit{niches} and focus on their manifestation, among others, on the social web. We analyze social data and perform user studies using the Accurator tool to understand what identifies a niche community, what indicates that a person is part of such a community and which properties identify a good candidate to provide qualitative annotations. 

The challenge for recommender strategies in Accurator is twofold: keep the expertise needed to annotate the item in the range of the experts' knowledge and yet diversify the suggestions to get high quality annotations for as many distinct items as possible. 
Our aim is to develop recommender strategies that use content patterns from the Linked Data cloud, resulting in a list of recommendations consisting of diverse items. 
We hypothesize that encountering diverse items to annotate will help keep the expert motivated.

We address issues of determining trust in the expert users and their contributed annotations by modeling the user reputation and tracking their expertise across various topics over time. We believe subjective logic is suitable to model the reputation of users and semantic similarity measures to track and update the users' expertise. Since there is no gold standard for evaluating the annotations, we must rely on a peer reviewing process and other mechanisms such as provenance of the annotation process.
% like tracking provenance of the annotation process such as usage of terms from vocabularies by the user, typing speed etc. We also investigate the different metrics which will help in identifying good behavior of the users. 

% The professional annotation of artworks is a complex process that requires familiarity with the used classification schemes and (art-)historical expert knowledge. Since both will mostly not be available in candidate users for nichesourcing projects,
Since external users are not familiar with professional classification schemes and expert knowledge the institutions target, a fourth main challenge is breaking down the annotation process into facile tasks that can be solved with little effort and without this kind of professional knowledge. %(suggested in \cite{He2013}).
We believe that the interface for such a system has to present the task in a straightforward way while motivating the users to spend the time contributing their knowledge. We investigate which design aspects and underlying mechanisms are responsible for the quality and quantity of tags added by users and how to visualize trust and personalization aspects.

