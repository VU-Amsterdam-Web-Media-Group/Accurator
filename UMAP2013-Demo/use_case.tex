\section{Research Challenges}
\label{use_case}

One of the challenges of nichesourcing is finding candidate annotators that will produce good quality annotations for collection items. Besides topical knowledge, properties like availability, willingness to help and being able to share or transfer knowledge are also important. We believe that persons part of a topical community have an active interest in the topic and might be willing to help and share knowledge related to that topic. These topical communities we call niches and manifest themselves, among others, on the Social Web. We will analyze social data and perform user studies using the Accurator framework to understand what identifies a niche community, whether a person is part of such community and which properties identifies a good candidate to provide qualitative annotations. 

The challenge for recommender strategies in Accurator is twofold: keep the expertise needed to annotate the artwork in the range of the experts knowledge and yet diversify the suggestions to get high quality annotations for as many distinct artworks as possible. To address these challenges we will investigate the use of content patterns in the Linked Data cloud. Our aim is to develop recommender strategies that use these patterns, resulting in a list of recommendations consisting of diverse artworks. Using the alternative paths created by the patterns, items can be reached which reside in the long-tail. When experts are able to annotate these long-tail items, they will become more accessible in general. From a user perspective diversity is also important, we hypothesize that encountering diverse artworks to annotate will help keep the expert motivated.

We address issues of determining trust in the users and their contributed annotations by modeling the user reputation and tracking their expertise across various topics over time. We intend to use Subjective logic to model the reputation of users and semantic similarity measures to track and update the users expertise. Since there is no gold standard for evaluating the annotations, we must rely on peer reviewing process and other mechanisms like tracking provenance of the annotation process such as usage of terms from vocabularies by the user, typing speed etc. We also investigate the different metrics which will help in identifying good behavior of the users. 

The professional annotation of artworks is a complex process that requires familiarity with the used classification schemes and (art-)historical expert knowledge. 
Since both will mostly not be available in candidate users for nichesourcing projects, this process must be broken down into facile tasks that can be solved with little effort and without expert knowledge of classification schemes as suggested in \cite{He2013}.
The interface for such a system has to present the task in a straightforward way while motivating the users to contribute their knowledge and time.
To gain a better understanding of how to design such an interface, we investigate what design aspects and underlying mechanisms are responsible for the quality and quantity of tags added by users and how to visualize trust and personalization aspects.

